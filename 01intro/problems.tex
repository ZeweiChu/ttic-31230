\documentclass{article}
\input ../preamble
\parindent = 0em

\newcommand{\solution}[1]{}
%\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\centerline{\bf Problems For Fundamental Equations.}

\vfill
\vfill
Assume that probability distributions $P(y)$ are discrete with $\sum_y\;P(y) = 1$.

\bigskip
{\bf Problem 1:} The problem of population density estimation is defined by the following equation.
$$\Phi^* = \argmin_\Phi\;H(\mathrm{Pop},P_\Phi) = E_{y \sim \mathrm{Pop}}\;-\log\;P_\Phi(y)$$
This equation is used for language modeling --- estimating the probability distribution over the population of English sentences that appear, say, in the New York Times.

\medskip
(a) Show the following.
$$\Phi^* = \argmin_\Phi\;H(\mathrm{Pop},P_\Phi) = \argmin_\Phi \;KL(\mathrm{Pop},P_\Phi)$$

(b) Explain why we can measure $H(\pop,P_\Phi)$ but cannot measure $KL(\pop,P_\Phi)$.


\bigskip
{\bf Problem 2:} Consider the objective
\begin{equation}
  \label{revH}
  P^* = \argmin_P\;H(P,Q)
\end{equation}
Define $y^*$ by
$$y^* = \argmax_y\;Q(y)$$
Let $\delta_y$ be the distribution such that $\delta_y(y) = 1$ and $\delta_y(y') = 0$ for $y' \not = y$.
Show that $\delta_{y^*}$ minimizes (\ref{revH}).

Next consider
\begin{equation}
  \label{revKL}
  P^* = \argmin_P\;KL(P,Q)
\end{equation}
Show that $Q$ is the minimizer of (\ref{revKL}).

Next consider a subset $S$ of the possible values and let $Q_S$ be the restriction of $Q$ to the set $S$.
\begin{eqnarray*}
  Q_S(y) & = & \frac{1}{Q(S)}\left\{\begin{array}{ll} Q(y) & \mbox{for $y \in S$} \\ 0 & \mbox{otherwise} \end{array}\right.
\end{eqnarray*}
Show that that $KL(Q_S,Q) = -\ln Q(S)$, which will be quite small if $S$ covers much of the mass. Show that, in contrast, $KL(Q,Q_S)$ is infinite unless
$Q_S$ = $Q$.

When we optimize a model $P_\Phi$ under the objective $KL(P_\Phi,Q)$ we can get that $P_\Phi$ covers only one high probability region (a mode) of $Q$ (a problem called mode collapse)
while optimizing $P_\Phi$ under the objective $KL(Q,P_\Phi)$ we will tend to get that $P_\Phi$ covers all of $Q$.  The two directions are very different even though both
are minimized at $P = Q$.

\bigskip
{\bf Problem 5.}
Prove the data processing inequality that for any function $f$ with $z = f(y)$ we have $H(z) \leq H(y)$.

\bigskip
{\bf Problem 3:} Consider a joint distribution $P(x,y)$ on discrete random variables $x$ and $y$.
We define the marginal distributions $P(x)$ and $P(y)$ as follows.
\begin{eqnarray*}
  P(x) & = & \sum_y\;P(x,y) \\
  \\
  P(y) & = & \sum_x\;P(x,y)
\end{eqnarray*}
Let $Q(x,y)$ be defined to be the product of marginals.
$$Q(x,y) = P(x)P(y).$$
We define mutual information by
{\color{red} $$I(x,y) = KL(P,Q)$$}
which I will write as
{\color{red} $$I(x,y) = KL(P(x,y),Q(x,y))$$}
We define conditional entropy $H(y|x)$ by
{\color{red} $$H(y|x) = E_{x,y} \;-\log P(y|x).$$}

(a) Show
{\color{red} $$I(x,y) = H(y) - H(y|x) = H(x) - H(x|y)$$}

(b) Explain why (a) implies $H(x) \geq H(x|y)$.

\medskip
(c) By stating (b) conditioned on $z$ we have
$$H(x|z) \geq H(x|y,z).$$
Use this to show that the data process inequality applies to mutual information,
i.e., that for $z = f(y)$ we have $I(x,z) \leq I(x,y)$.

\bigskip
{\bf Problem 4:}
(a) For three distributions $P$, $Q$ and $G$ show the following equality.
$$KL(P,Q) =  \left(E_{y \sim P} \;\log \frac{G(y)}{Q(y)}\right) + KL(P,G)$$

(b) Show that this implies
$$KL(P,Q) =  \sup_G \;E_{y \sim P}\; \log \frac{G(y)}{Q(y)}$$

(c) Now define
\begin{eqnarray*}
  G(y) & = & \frac{1}{Z}\;Q(y)e^{s(y)} \\
  \\
  Z & = & \sum_y \;Q(y)e^{s(y)}
\end{eqnarray*}
Show that a distribution $G(y)$ which does not assign zero to any point can be represented by a score $s(y)$ and that under this
change of variables we have
$$KL(P,Q) =  \sup_s \;E_{y \sim P}\; s(y) - \log E_{y \sim Q} \;e^{s(y)}$$
This is the Donsker-Varadhan variational representation of KL-divergence.
This can be used in cases where we can sample from $P$ and $Q$ but cannot compute $P(y)$ or $Q(y)$.
Instead we can use a model score $s_\Phi(y)$ where $s_\Phi(y)$ can be computed.

\ignore{
\bigskip
Problems in continuous information theory.

\bigskip
{\bf Problem 6.}
Calculate the differential entropy of a Gaussian distribution
$$p(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-x^2}{2\sigma^2}}.$$
Use the natural logarithm in your definition of entropy.

\solution{
\begin{eqnarray*}
H(p) & = & E_{x \sim P} -\ln p(x) \\
\\
& = & E_{x \sim P}\; \frac{x^2}{2 \sigma^2} + \ln \sigma + \ln \sqrt{2\pi} \\
\\
& = & \frac{\sigma^2}{2\sigma^2} + \ln \sigma + \frac{1}{2}\ln 2\pi \\
\\
& = & \ln \sigma + \frac{1}{2}(1 + \ln 2\pi)
\end{eqnarray*}
}

\bigskip
{\bf Problem 7.} Let the ``signal'' $x$ be a Gaussian random variable with variance $\sigma_x$ and let the ``noise'' $\epsilon$ be an independent Gaussian random variable with
variance $\sigma_\epsilon$.  Let $z = x + \epsilon$.  Use the fact that a sum of independent Gaussians is Gaussian with $\sigma^2_z = \sigma^2_x + \sigma^2_\epsilon$
to compute the differential mutual information $I(x,z)$.  Express your answer in terms of the signal to noise ratio $\sigma^2_x/\sigma^2_\epsilon$.
Hint: select a convenient expression for mutual information and use the answer to problem 5.

\solution{
\begin{eqnarray*}
I(z,x) & = & H(z) - H(z|x) \\
\\
& = & \ln (\sigma^2_x + \sigma^2_\epsilon) - \ln \sigma^2_\epsilon \\
\\
& = & \ln \left(\frac{\sigma^2_x + \sigma^2_\epsilon}{\sigma^2_\epsilon}\right) \\
\\
& = & \ln \left(1 + \frac{\sigma^2_x}{\sigma^2_\epsilon} \right)
\end{eqnarray*}
}

\bigskip
{\bf Problem 8.} For both the differential entropy in problem 5,  and the mutual information in problem 6, say whether the numerical value depends on the choice of units.

\solution{
          The numerical value of differential entropy is sensitive to the units we choose for $\sigma$.  As long as $\sigma_x$ and $\sigma_\epsilon$
          are measured in the same units, the numerical value of the mutual information is units-independent. This is consistent with the fact
          that differential entropy is not directly meaningful while mutual information can be written as a KL-divergence and differential KL-divergence
          is meaningful.
          }

\bigskip
{\bf Problem 9.} Calculate the KL divergence between one dimensional Gaussians with the same variance but different means.

}

\end{document}
