<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> Stochastic Gradient Descent (SGD) </p>

<p><a href = SGD.pdf> Slides</a></p>

<p><a href = SGDproblems.pdf> Problems</a></p>

<p><a href = http://ruder.io/optimizing-gradient-descent/> Blog post on SGD variants</a></p>

<p><a href = https://arxiv.org/abs/1706.02677> Training Resnt-50 on Imagenet in one hour</a></p>

<p><a href = https://arxiv.org/abs/1709.08728> Some theory on scaling learning rate with batch size </a></p>

<p><a href = https://arxiv.org/abs/1511.06807> Adding Gradient Noise</a></p>

<p><a href = https://arxiv.org/abs/1704.00109> Temperature Cycling in SGD </a></p>

<p><a href = https://arxiv.org/abs/1206.1901> MCMC with momentum</a></p>






  
