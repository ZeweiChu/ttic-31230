\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning, winter 2019}
\medskip
\centerline{\bf Backpropagation  Problems}

\bigskip
{\bf Problem 1:} Consider the following softmax.
\begin{eqnarray*}
  Z[b] & = & \sum_j\;\exp(s[b,j]) \\
  p[b,j] & = & \exp(s[b,j])/Z[b]
\end{eqnarray*}

An alternative way to compute this is to initialize the tensors $Z$ and $p$ to zero and then execute the following loops.

\medskip
$\mathrm{for}\;b,j$
$\;\;\;Z[b]\;\pluseq\;\exp(s[b,j])$

\medskip
$\mathrm{for}\;b,j$
$\;\;\;p[b,j] \;\pluseq\; \exp(s[b,j])/Z[b]$

\medskip
Each individual $\pluseq$ operation inside the loops can be treated independently in backpropagation.

\medskip
(a) Give a back-propagation loop over $\pluseq$ updates based on the second loop for adding to $s.\grad$ using $p.\grad$
(and using the forward-computed tensors $Z$ and $s$).

\solution{
  \medskip
  For $b,j\;\;\;s.\grad[b,j] \;\pluseq\; p.\grad[b,j]\exp(s[b,j])/Z[b]$}

\medskip
(b) Give a back-propagation loop over $\pluseq$ updates based on the second equation for adding to $Z.\grad$ using $p.\grad$
(and using the forward-computed tensors $s$ and $Z$).

\solution{
  \medskip
  For $b,j\;\;\;Z.\grad[b] \;\minuseq\; p.\grad[b,j]\exp(s[b,j])/Z[b]^2$}

\medskip
(c) Give a back-propagation loop over $\pluseq$ updates based on the first equation for adding to $s.\grad$ using $Z.\grad$
(and using the forward-computed tensor $s$).

\solution{
  \medskip
  For $b,j\;\;\;s.\grad[b,j] \;\pluseq\; Z.\grad[b]\exp(s[b,j])$}

\bigskip
{\bf Problem 2:} Show that the addition to $s.\grad$ shown in problem 1 can be computed using the following more efficient updates.

\medskip
$\mathrm{for}\;b,j\;\;\;e[b] \; \minuseq \; p[b,j]p.\grad[b,j]$

\medskip
$\mathrm{for}\;b,j\;\;\;s.\grad[b,j] \;\pluseq \; p[b,j](p.\grad[b,j] + e[b])$

\solution{
  The updates for problem 1 can be written as

  \medskip
  \begin{eqnarray*}
    \mathrm{for}\;b\;\;\;Z.\grad[b] & = & \sum_j \;-p.\grad[b,j]\exp(s[b,j])/Z[b]^2 \\
    & = & \left(\sum_j -p[b,j]p.\grad[b,j]\right)/Z[b] \\
    & = & e[b]/Z[b]
  \end{eqnarray*}

\begin{eqnarray*}
  \mathrm{for}\;b,j\;\;\;s.\grad[b,j] & = & p.\grad[b,j]\exp(s[b,j])/Z[b] + Z.\grad[b]\exp(s[b,j]) \\
  & = &  p.\grad[b,j]\left(\exp(s[b,j])/Z[b]\right) + e[b]\left(\exp(s[b,j])/Z[b]\right) \\
    & = &  p[b,j](p.\grad[b,j] +e[b])
\end{eqnarray*}

}
  
\medskip
This formula shows how hand-written back-propagation methods for ``layers'' such as softmax
can be more efficient than compiler-generated back-propagation code.
While optimizing compilers can of course be written, one must keep in mind the trade-off
between the abstraction level of the programming language and the efficiency of the generated code.

\medskip
{\bf Problem 3.} Consider the following set of $\pluseq$ statements defining batch normalization
where all computed tensors are initialized to zero.

\medskip
For $b,j \;\;\mu[j] \;\pluseq\; \frac{1}{B}\;x[b,j]$

\medskip
For $b,j\;\;s[j] \;\pluseq\; \frac{1}{B-1}\;(x[b,j] - \mu[j])^2$

\medskip
For $b,j\;\;x'[b,j] \;\pluseq\; \frac{x[b,j] - \mu[b]}{\sqrt{s[j]}}$

\medskip
Give backpropagation $\pluseq$ equations for computing $x.\grad[b,j]$, $\mu.\grad[j]$, and $s.\grad[j]$ from $x'.\grad[b,j]$.


\bigskip
{\bf Problem 4.} The equations defining a UGRNN are given below.


\begin{eqnarray*}
R_t[b,j] & = & \mathrm{tanh}\left(\left(\sum_i\;W^{h,R}[i,j]{\color{red} h_t[b,i]}\right) + \left(\sum_k W^{x,R}[k,j]{\color{red} x_t[b,k]}\right) - B^R[j]\right) \\
\\
G_t[b,j] & = & \sigma\left(\left(\sum_i\;W^{h,G}[i,j]{\color{red} h_t[b,i]}\right) + \left(\sum_k W^{x,G}[k,j]{\color{red} x_t[b,k]}\right) - B^G[j]\right) \\
\\
{\color{red} h_{t+1}[b,j]} & = & G_t[b,j]{\color{red} h_t[b,j]} + (1-G_t[b,j])R_t[b,j]
\end{eqnarray*}

(a) Rewrite this using $\pluseq$ loops instead of summations assuming that all computed tensors are initialized to zero.
You will need to define two additional tensors for the inputs to the activation functions.

\medskip
(b) Give $\pluseq$ loops for the backward computation of tensor gradients starting from $h_{t+1}.\grad[B,J]$.  You can write the derivatives of the activation functions tanh and $\sigma$
simply as $\mathrm{tanh}'$ and $\sigma'$ respectively.

    
\end{document}
