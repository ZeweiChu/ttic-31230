\documentclass{article}
\input ../preamble
\parindent = 0em
\parskip = 1ex

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}

\bigskip

\centerline{\bf Problems For Fundamental Equations.}

\bigskip
\bigskip
{\bf problem 1.} This problem is on a CTC-like algorithm for image labeling.

Suppose that the training data consists of pairs $(I[X,Y],S)$ where $I$ is an image and $S$
is a set of object types occuring in the image.  For example $S$ might be $\{\mathrm{Person},\mathrm{Dog},\mathrm{Car}\}$.
To be concrete we can take ${\cal C}$ to be the set of image labels used in CIFAR 100 and take $S$ to be a subset
of ${\cal C}$ containing no more than five labels ($|S| \leq 5$).
We want to do SGD on a model defining $P_\Phi(S\;|\;I[X,Y])$.

We will use a latent variable $z[X,Y]$ such that for pixel coordinates $(x,y)$ we have $z[x,y] \in {\cal C} \cup \{\bot\}$.
For a given $z[X,Y]$ define $S(z[X,Y])$ to be the set of classes appearing in $z[X,Y]$, i.e.,
$S(z[X,Y]) = \{c\;\exists\;x,y\;z(x,y) = c\}$. Here the ``semantic segmentation'' $Z[X,Y]$ is analogous to the phoneme
sequence $z[T]$ in CTC. Unlike the CTC model, the label
$S$ is a set rather than a sequence.

We assume a CNN (with convolutions of stride 1 to preserve spatial dimensions) followed by
a softmax at each pixel to get a probability
$P_\Phi(z[x,y] = c)$ for each pixel location $(x,y)$ and each $c \in {\cal C} \cup \{\bot\}$ and where
each pixel location has an independent probability distribution over classes. 
To simplify notation we can reshape the pixel locations into a linear sequence
and replace $z[X,Y]$ by $z[T]$ with $T = X \times Y$.

Define
$$S_t = \{c \in {\cal C}\;\exists t\leq T\;z[t] = c\}$$
For $U \subseteq S$ define
$$F[U,t] = P(S_t = U)$$
Note that for $|S| \leq 5$ there are at most 32 possible values of $U$.
Give dynamic programming equations defining $F[U,0]$ and defining $F[U,t+1]$ in term of $F[U,t]$.

\ignore{
{\bf Problem}
This problem is on Pseudolikelihood. Consider a graphical model with $N$ nodes numbered 1 through $N$ and where each node can take on one of the values $0$ or $1$.
We let $\hat{x}$ be an assignment of a value to every node. We define the score of $\hat{x}$ by
$$f(\hat{x}) = \sum_{i = 1}^{N-1} \mathbbm{1}[\hat{x}[i] = \hat{x}[i+1]]$$
The probability distribution over assignments is defined by a softmax.
$$Q_f(\hat{x}) = \softmax_{\hat{x}} f(\hat{x})$$
What is the {\bf Pseudoliklihood} of the all ones assignment?

\solution{
$$\tilde{P}_f(\hat{x}) = \Pi_i P_f(\hat{x}[i] \;|\; \hat{x}/i)$$

where $\hat{x}/i$ consists of all components of $\hat{x}$ other than $i$.  In a graphical model $P_f(\hat{x}[i] \;|\; \hat{x}/i)$ is determined by the neighbors of $i$ and we can consider only how a value is scored against it neighbors.  For $\hat{x}$ equal to all ones we have

\begin{eqnarray*}
f(\hat{x}) & = & N-1 \\
\\
f(\hat{x}[i = 0]) & = & \left\{\begin{array}{ll} N-3 & \mbox{for $1 < i < N$} \\ N-2 & \mbox{for $i=1$ or $i=N$} \end{array}\right.
\end{eqnarray*}
For $1 < i < N$ we get
\begin{eqnarray*}
Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) & = & \frac{e^{N-1}}{e^{N-1} + e^{N-3}} \\
\\
& = & \frac{1}{1 + e^{-2}}
\end{eqnarray*}
and for $i = 1$ or $i = N$ we get
$$Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) = \frac{1}{1 + e^{-1}}$$

This gives

$$\tilde{Q}(\hat{x}) = (1 + e^{-1})^{-2}(1 + e^{-2})^{-(N-2)}$$
}}



  
\end{document}
