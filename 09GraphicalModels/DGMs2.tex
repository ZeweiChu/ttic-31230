\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2018}
  \vfill
  \centerline{\bf Deep Graphical Models II}
  \vfill
    \centerline{\bf Algorithms for Approximate SGD}
\vfill


\slide{Consider Colorization}

\centerline{\includegraphics[height = 2in]{../images/colorization}}

$x$ is a black and white image.

\vfill
$y$ is a color image drawn from $\pop(y|x)$.

\vfill
$\hat{y}$ is an arbitrary color image.

\vfill
$Q_\Phi(\hat{y}|x)$ is the probability that model $\Phi$ assigns to the color image $y$ given black and white image $x$.

\slide{Exponential Softmax}

$$f(\hat{y}) = \sum_{\alpha \in \mathrm{HyperEdges}}  \; f[\alpha,\hat{y}[\alpha]]$$

\begin{eqnarray*}
\Phi^* & = &  \argmin_\Phi \;\;E_{(x,y) \sim \mathrm{Pop}}  -\log Q_{f_\Phi(x)}(y) \\
 \\
 \\
    f.\mathrm{grad}[\alpha,\tilde{y}]
    & = &  {\color{red} P_{\hat{y} \sim Q_f}(\hat{y}[\alpha] = \tilde{y})} - \bbone[y[\alpha] = \tilde{y}] \\
\end{eqnarray*}

\vfill
The quantities ${\color{red} P_{\hat{y} \sim Q_f}(\hat{y}[\alpha] = \tilde{y})}$ are {\bf sufficient statistics}.

\slide{Estimation by Sampling}

We can estimate $P_{\hat{y} \sim Q_f}(\hat{y}[\alpha] = \tilde{y})$ by sampling $\hat{y}$

\vfill
Hinton has proposed that this is the function of dreaming.

\slide{Monte Carlo Markov Chain (MCMC) Sampling}

\centerline{\bf Metropolis Algorithm}


\vfill
Pick an initial graph label $\hat{y}$ and then repeat:

\begin{enumerate}
\item Pick a ``neighbor'' $\hat{y}'$ of $\hat{y}$ uniformly at random.  The neighbor relation must be symmetric.  Perhaps Hamming distance one.

  \vfill
\item If $f(\hat{y}') > f(\hat{y})$ update $\hat{y} = \hat{y}'$

  \vfill
\item If $f(\hat{y}') \leq f(\hat{y})$ then update $\hat{y} = \hat{y}'$ with probability $e^{-(f(\hat{y}) - f(\hat{y}'))}$
  \end{enumerate}  

\slide{Markov Processes and Stationary Distributions}

A Markov process is a process defined by a fixed state transition probability $P(\hat{y}'|\hat{y}) = M_{\hat{y}',\hat{y}}$.

\vfill
Let $P^t$ the probability distribution for time $t$.

\vfill
$$P^{t+1} = MP^t$$

\vfill
If every state can be reached form every state (ergodic process) then $P^t$ converges to a unique {\bf stationary distribution} $P^\infty$

\vfill
$$P^\infty = MP^\infty$$

\slide{Metropolis Correctness}

To verify that the Metropolis process has the correct stationary distribution we simply verify that $MP = P$ where $P$
is the desired distribution.

\vfill
This can be done by checking that under the desired distribution the flow from $\hat{y}$ to $\hat{y}'$
equals the flow from $\hat{y}'$ to $\hat{y}$ ({\bf detailed balance}).

\slide{Metropolis Correctness}

For $f(\hat{y}) \geq f(\hat{y}')$

\vfill
\begin{eqnarray*}
  \mathrm{flow}(\hat{y}' \rightarrow \hat{y}) &  = & \frac{1}{Z}e^{f(\hat{y}')} \frac{1}{N} \\
  \\
\mathrm{flow}(\hat{y} \rightarrow \hat{y}') & = & \frac{1}{Z}e^{f(\hat{y})} \frac{1}{N} e^{-\Delta f} = \frac{1}{Z} e^{f(\hat{y}')} \frac{1}{N}
\end{eqnarray*}

\vfill
But detailed balance is not required in general (see Hamiltonian MCMC).

\slide{Gibbs Sampling}

The Metropolis algorithm wastes time by rejecting proposed moves.

\vfill
Gibbs sampling avoids this move rejection.

\vfill
In Gibbs sampling we select a node $i$ at random and change that node by drawing a new node value conditioned on the current values of the other nodes.

\slide{Gibbs Sampling}

\begin{eqnarray*}
  Q_f(i=\tilde{y} \;|\;\hat{y}) & \doteq & Q_f(\hat{y}[i] = \tilde{y}\;|\;\hat{y}[1],\ldots,\hat{y}[i-1],\hat{y}[i+1],\ldots,\hat{y}[I])
\end{eqnarray*}

\vfill
Markov Blanket Property:
$$Q_f(i=\tilde{y} \;|\;\hat{y}) = Q_f(i=\tilde{y} \;|\;\hat{y}[N(i)])$$
  
\vfill
Gibbs Sampling, Repeat:

\begin{itemize}
\item   Select $i$ at random

\item draw $\tilde{y}$ from $Q_f(i = \tilde{y} \;|\;\hat{y})$

\item $\hat{y}[i] = \tilde{y}$
\end{itemize}

\slide{Gibbs Sampling}

Let $\hat{y}[i = \tilde{y}]$ be the assignment $\hat{y}'$ equal to $\hat{y}$ except $\hat{y}'[i] = \tilde{y}$.

\vfill
\begin{eqnarray*}
 Q_f(i = \tilde{y} \;|\; \hat{y})  & = & \frac{Q_f(\hat{y}[i] = \tilde{y})}{\sum_{\tilde{y}} Q_f(\hat{y}[i] = \tilde{y})} \\
  \\
  \\
  & = & \frac{e^{f(\hat{y}[i = \tilde{y}])}}{\sum_{\tilde{y}} e^{f(\hat{y}[i = \tilde{y}])}}
\end{eqnarray*}

\slide{Correctness Proof}

$Q_f(\hat{y})$ is a stationary distribution of Gibbs Sampling.

\vfill
\begin{itemize}
\item   Select $i$ at random

\item draw $\tilde{y}$ from $Q_f(i = \tilde{y} \;|\;\hat{y})$

\item $\hat{y}[i] = \tilde{y}$
\end{itemize}


\vfill
The distribution before the update equals the distribution after the update.

\slide{Pseudolikelihood}

In Pseudolikelihood we replace the objective $- \log Q_f(\hat{y})$ with the objective $- \log \tilde{Q}_f(\hat{y})$ where

\vfill
\begin{eqnarray*}
  \tilde{Q}_f(\hat{y}) & \doteq & \prod_i \;Q_f(i = \hat{y}[i] \;|\;\hat{y}) \\
  \\
  \mathrm{loss}(f) & \doteq & - \log \tilde{Q}(y) \\
  \\
  f.\mathrm{grad}[\alpha,\tilde{y}] & = & \sum_i - \partial \log Q_f[i = \hat{y}[i] \;|\;\hat{y}] /\partial f[\alpha,\tilde{y}]
\end{eqnarray*}


\slide{Pseudolikelihood Consistency}

$$\argmin_Q \; E_{y \sim \mathrm{Pop}} \;-\log \tilde{Q}(y) = \mathrm{Pop}$$

\vfill

\slide{Proof of Consistency I}

We have

$$\min_{Q} \;E_{y \sim \mathrm{Pop}}\;-\log \tilde{Q}(y) \;\;\leq \;\; E_{y \sim \mathrm{Pop}}\;-\log \widetilde{\mathrm{Pop}}(y)$$

\vfill
If we can show

$$\min_{Q} \;E_{y \sim \mathrm{Pop}}\;-\log \tilde{Q}(y) \;\;\geq \;\; E_{y \sim \mathrm{Pop}}\;-\log \widetilde{\mathrm{Pop}}(y)$$

Then the minimizer (the argmin) is $\mathrm{Pop}$ as desired.

\slide{Proof of Consistency II}

We will prove the case of two nodes.

\vfill
\begin{eqnarray*}
  & & \min_Q \;E_{y\sim \mathrm{Pop}}{-\log Q(y[1]|y[2])\;Q(y[2]|y[1])} \\
  \\
  & \geq & \min_{Q_1,Q_2} E_{y \sim \mathrm{Pop}}{-\log Q_1(y[1]|y[2])\;Q_2(y[2]|y[1])} \\
  \\
  & = & \min_{Q_1} E_{y \sim \mathrm{Pop}}{-\log Q_1(y[1]|y[2])} + \min_{Q_2} E_{y \sim \mathrm{Pop}}{-\log Q_2(y[2]|y[1])} \\
  \\
  & = & E_{y \sim \mathrm{Pop}}{-\log \mathrm{Pop}(y[1]|y[2])} + E_{y \sim \mathrm{Pop}}{-\log \mathrm{Pop}(y[2]|y[1])} \\
  \\
  & = & E_{y \sim \mathrm{Pop}}{-\log \widetilde{\mathrm{Pop}}(y|x)}
\end{eqnarray*}

  
\slideplain{Contrastive Divergence}
{\bf Algorithm (CDk)}: Run $k$ steps of MCMC for $Q_f(\hat{y})$ {\bf starting from $y$} to get $\hat{y}$.

\vfill
Then set
$$f.\mathrm{grad}[\alpha,\tilde{y}] = \mathbbm{1}[\hat{y}[\alpha] = \tilde{y}] - \mathbbm{1}[y[\alpha]= \tilde{y}]$$

\vfill
    {\bf Theorem}: If $Q_f(\hat{y}) = \mathrm{Pop}$ then
    
    $$E_{y \sim \mathrm{Pop}}\; \mathbbm{1}[\hat{y}[\alpha] = \tilde{y}] - \mathbbm{1}[y[\alpha]= \tilde{y}] = 0$$

\vfill
{\bf Here we can take $k=1$ --- \bf no mixing time required}.


\slide{END}

}
\end{document}

