\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2019}
  \vfill
  \centerline{\bf Connectionist Temporal Classification (CTC)}
  \vfill
  \centerline{\bf and Deep Graphical Models}
\vfill
\vfill
\vfill
\slidetwo{The Fundamental Equation:}
{Conditional vs. Unconditional}

\vfill
{\color{red}
$$\Phi^* = \argmin_\Phi E_{(x,y) \sim \mathrm{Pop}}\;-\ln \;P(y|x)$$

\vfill
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Pop}}\;-\ln \;P(y)$$
}

\vfill
This is a non-distinction: the analysis of the conditional case
is exactly the same as that of the unconditional case.

\slidetwo{The Fundamental Equation:}
{Distributions on Exponentially Large Sets}

{\color{red} The structured case:} $y \in {\cal Y}$ where ${\cal Y}$ is discrete but {\color{red} iteration over $\hat{y} \in {\cal Y}$ is infeasible}.

\vfill
Language modeling (unconditional) and machine translation (conditional) are distributions on exponentially large (even infinite) sets.

\slide{Friendly and Unfriendly Distributions}

A model $P_\Phi(y)$ will be called {\color{red} friendly} if we can efficiently sample from it and, for any given $y$, can efficiently compute $P_\Phi(y)$.

\vfill
{\color{red} Autoregressive} language models (unconditional) and autoregressive machine translation models (conditional) are {\color{red} friendly}.


\vfill
Distributions which are not friendly in this sense will be called {\color{red} unfriendly}.

\slide{The Importance of Being Friendly}


In cases where $P_\Phi(y|x)$ can be computed (a friendly model)
we can do SGD by back-propagating through the computation of $P_\Phi(y|x)$.



\slidetwo{Graphical Models are Generally Unfriendly:}
{Semantic Segmentation}
\centerline{\includegraphics[height = 2.5in]{../images/SLICcolor}}
\centerline{\huge SLIC superpixels, Achanta et al.}

\vfill
We want to assign each superpixel one of $k$ semantic classes.

\vfill
For example ``person'', ``car'', ``building'', ``sky'' or ``other''.

\anaslide{Exponential Softmax}
\bigskip
{\color{red}
\begin{eqnarray*}
P_\Phi(\hat{y}|x) & = & \expsoftmax_{\hat{y}}\;s_\Phi(\hat{y}|x)
\end{eqnarray*}
}

\vfill
Let ${\cal C}$ be the semantic classes, ${\cal I}$ be superpixels, and ${\cal E}$ be edges.

\vfill
We will compute

\vfill {\bf a unary potential tensor} $s_i[c] = s_\Phi(c|x,i)$

\vfill
{\bf a binary potential tensor} $s_e[c,c'] = s_\Phi(c,c'|x,e)$

\vfill
{\color{red} $$s_\Phi(\hat{y}|x) = \sum_{i \in {\cal I}} s_i[\hat{y}[i]] + \sum_{e \in {\cal E}}\;s_e[\hat{y}[e.i],\hat{y}[e.j]]$$}

\anaslide{Exponential Softmax}
\bigskip
{\color{red}
\begin{eqnarray*}
P_\Phi(\hat{y}|x) & = & \expsoftmax_{\hat{y}}\;s_\Phi(\hat{y}|x)
\end{eqnarray*}
}

\vfill
Distributions defined by exponential softmax operations are not friendly in general --- there is no efficient general sampling algorithm or efficient general method of computing $Z$.

\vfill
Computing $Z$ is easily shown to be \#P hard.

\slide{Latent Variables}

We are often interested in models of the form

\vfill
{\color{red} $$P_\Phi(y) = \sum_z\;P_\Phi(z)P_\Phi(y|z).$$}

\vfill
Probabilistic grammar models have this form where $y$ is a sentence and $z$ is a parse tree
and $P(y|z)$ is deterministic.


\slide{Exponential Softmax as Intermediate Computation}
$$P_\Phi(y) = \sum_z\;P_\Phi(z)P_\Phi(y|z).$$

\vfill
{\color{red}
\begin{eqnarray*}
& & \mbox{intput}\; x \\
\vdots \\
z & = & \expsoftmax_z \ldots \\
\\
& & \mbox{input}\; z \\
\vdots \\
y & = & \expsoftmax_y\;\ldots
\end{eqnarray*}
}

\slide{A Composition of Friendlies is Typically  Unfriendly}

{\color{red} $$P_\Phi(y) = \sum_z\;P_\Phi(z)P_\Phi(y|z).$$}

\vfill
It is often the case that $P_\Phi(z)$ is friendly, and $P_\Phi(y|z)$ is friendly, but $P_\Phi(y)$ is not friendly (the sum over $z$ is intractible).

\vfill
For example $z$ might be uniformly distributed over assignments of truth values to Boolean variables (which is friendly) and $y$ might be the value of a fixed Boolean formula $\Phi$ (which is friendly given $z$).  In this case
determining if $P_\Phi(y) > 0$ is the SAT problem which is NP hard.

\slidetwo{Connectionist Temporal Classification (CTC)}
{A Successful Friendly Latent Variable Model}

A speech signal
$$x = x_1,\; \ldots,\; x_{\color{red} T}$$
is labeled with a phone sequence
$$y = y_1, \ldots, y_{\color{red} N}$$
with {\color{red} $N << T$} and with $y_n \in {\cal P}$ for a set of phonemes ${\cal P}$.

\vfill
{\color{red} The length $N$ of $y$ is not determined by $x$ and the alignment between $x$ and $y$ is not given.}

\slide{CTC: A Friendly Compositions of Friendlies}

{\color{red} $$P_\Phi(y|x) = \sum_z\;P_\Phi(z|x)P_\Phi(y|z).$$}

\vfill
\begin{tabbing}
Input Signal: \hspace{3em} \=$x = x_1,\;\ldots,\;x_{\color{red} T}$ \\
Latent Label: \>$z = z_1,\;\ldots,\;z_{\color{red} T},\;\;\;z_t \in {\cal P} \cup \{\bot\}$ \\
Output: \>$y(z) = y_1,\;\ldots,\;y_{\color{red} N}$
\end{tabbing}

\vfill
$y(z)$ is the result of removing all the occurrences of $\bot$ from $z$:

{\color{red} $$\hspace{5em} z \hspace{7em} \Rightarrow \hspace{2em} y$$}
{\color{red} $$\bot,a_1,\bot,\bot,\bot,a_2,\bot,\bot,a_3,\bot \;\;\;\Rightarrow\;\; a_1,a_2,a_3$$}


\slide{The CTC Model}

For $z \in {\cal P} \cup \{\bot\}$ we have an embedding $e(z)$.  The embedding is a parameter of the model.

\begin{eqnarray*}
  h_1,\;\ldots,\;h_T & = & \mathrm{RNN}_\Phi(x_1,\;\ldots,\;x_T) \\
  \\
  P_\Phi(z_t|x_1,\ldots,x_T) & = & {\color{red} \softmax_{z} \;e(z)^\top h_t}
\end{eqnarray*}

\vfill
$z_1$, $\ldots$ $z_T$ are {\color{red}  all independent} given $x$ (very friendly).

\vfill
$P_\Phi(y|z)$ is {\color{red} deterministic} (very friendly).

\vfill
{\color{red} But it is not obvious whether $P_\Phi(y|x)$ is friendly.}


\slide{Dynamic Programming (Forward-Backward)}
\begin{eqnarray*}
  x & = & x_1,\;\ldots,\; x_T \\
  z & = & z_1, \;\ldots,\;z_T,\;\;\;z_t \in {\cal P}\cup \{\bot\} \\
  y & = & y_1,\;\ldots,\;y_N,\;\;\;\;y_n \in {\cal P},\;\;\;N << T \\
  y(z) & = & (z_1,\;\ldots,\;z_T) - \bot
\end{eqnarray*}

\vfill

Forward-Backward
\begin{eqnarray*}
  \vec{y}_t & = & (z_1,\;\ldots,\;z_t)-\bot \\
  {\color{red} F[n,t]} & = & {\color{red} P(\vec{y}_t = y_1,\;\ldots,\;y_n)} \\
  {\color{red} B[n,t]} & = & {\color{red} P(y_{n+1},\;\ldots,y_N | \vec{y}_t = y_1,\;\ldots,\;y_n)} \\
  P(y) & = & F[N,T] = B[0,0]
\end{eqnarray*}



\slide{Dynamic Programming (Forward-Backward)}

\begin{eqnarray*}
  \vec{y}_t & = & (z_1,\;\ldots,\;z_t)-\bot \\
  {\color{red} F[n,t]} & = & {\color{red} P(\vec{y}_t = y_1,\;\ldots,\;y_n)} \\
  {\color{red} B[n,t]} & = & {\color{red} P(y_{n+1},\;\ldots,y_N | \vec{y}_t = y_1,\;\ldots,\;y_n)}
\end{eqnarray*}

\begin{eqnarray*}
  F[0,0] & = & 1 \\
  F[n,0] & = & 0 \;\;\;\mbox{for $n > 0$} \\
  {\color{red} F[n+1,t+1]} & = & {\color{red} P(z_{t+1} = \bot) F[n+1,t] + P(z_{t+1} = y_{n+1})F[n,t]} \\
  \\
  B[N,T] & = & 1 \\
  B[n,T] & = & 0,\;\;\;\mbox{for $n < N$} \\
  {\color{red} B[n-1,t-1]} & = & {\color{red} P(z_t = \bot)B[n-1,t] +  P(z_t = y_n)B[n,t]}
\end{eqnarray*}

\slideplain{Graphical Models}

\centerline{\includegraphics[height = 4in]{../images/Koller}}
\centerline{Koller and Friedman, MIT Press, 2009, 1270 pages}

\slidetwo{Unfriendly Exponential Softmax:}
{Semantic Segmentation}
\centerline{\includegraphics[height = 2.5in]{../images/SLICcolor}}
\centerline{\huge SLIC superpixels, Achanta et al.}

\vfill
We want to assign each superpixel one of $k$ semantic classes.

\vfill
For example ``person'', ``car'', ``building'', ``sky'' or ``other''.


\anaslide{Unfriendly Exponential Softmax}
\bigskip
{\color{red}
\begin{eqnarray*}
P_\Phi(\hat{y}|x) & = & \expsoftmax_{\hat{y}}\;s_\Phi(\hat{y}|x)
\end{eqnarray*}
}

\vfill
Let ${\cal C}$ be the semantic classes, ${\cal I}$ be superpixels, and ${\cal E}$ be edges.

\vfill
We will compute

\vfill {\bf a unary potential tensor} $s_i[c] = s_\Phi(c|x,i)$

\vfill
{\bf a binary potential tensor} $s_e[c,c'] = s_\Phi(c,c'|x,e)$

\vfill
{\color{red} $$s_\Phi(\hat{y}|x) = \sum_{i \in {\cal I}} s_i[\hat{y}[i]] + \sum_{e \in {\cal E}}\;s_e[\hat{y}[e.i],\hat{y}[e.j]]$$}

\slide{Back-Propagation Through Unfriendly Softmax}

\vfill
\begin{eqnarray*}
\mbox{intput}\; x \\
 & \vdots & \\
{\color{red} s_i[c]} & = & \ldots \\
{\color{red} s_e[c,c']} & = & \ldots \\
{\cal L} & = & - \ln\; P(y\;|\;s_{\cal I}[{\cal C}],\;s_{\cal E}[{\cal C}, {\cal C}])
\end{eqnarray*}

\vfill
We need to compute {\color{red} $s_i.\grad[c]$} and {\color{red} $s_e.\grad[c,c']$}.

\slide{General Markov Random Fields (MRFs)}

\centerline{\includegraphics[height= 1.5in]{../images/Graph}}

$$s(\hat{y}) = \sum_{i \in \mathrm{Nodes}}\; s_i[\hat{y}[i]]\; + \sum_{e \in \mathrm{Edges}}\;s_e[\hat{y}[e.i],\hat{y}[e.j]]$$

\vfill
\centerline{Node Potentials \hspace{4em}Edge Potentials}

\slide{An Example}

Consider an image with three superpixels $A$, $B$ and $C$ where
each superpixel is to labeled as either ``foreground'' or background.

\vfill
Suppose the unary potentials are all zero.

\vfill
$$s_A(\mathrm{Foreground}) = s_A(\mathrm{Background}) = 0$$
$$s_B(\mathrm{Foreground}) = s_B(\mathrm{Background}) = 0$$
$$s_C(\mathrm{Foreground}) = s_C(\mathrm{Background}) = 0$$

\slide{The Binary Potentials}


\vfill
Let $F_A$ be the proposition that $A$ is forground and similarly for $F_B$ and $F_C$.

\vfill
We can express $P_A \Rightarrow P_B$ with
$$s_{A,B}(\mathrm{Foreground},\mathrm{Background}) = -1$$
$$s_{A,B}(\mathrm{Foreground},\mathrm{Foreground}) = 1$$
$$s_{A,B}(\mathrm{Background},\mathrm{Background}) = 1$$
$$s_{A,B}(\mathrm{Background},\mathrm{Foreground}) = 1$$

\vfill
The binary potentials are then given by
$F_A \Rightarrow F_B$, $F_B \Rightarrow F_C$, $F_C \Rightarrow F_A$.

\slide{The Full Configuration Potential}

For any configuration $\hat{y}$ we have that $s(\hat{y})$ is the sum of the unary and binary potentials.

\vfill
If none are foreground we have $s(\hat{y}) = 3$

\vfill
If one is foreground we have $s(\hat{y}) = -1 + 1+ 1 = 1$

\vfill
If two are foreground we also have $s(\hat{y}) = -1 + 1+ 1 = 1$

\vfill
If all are foreground we have $s(\hat{y}) = 3$.

\vfill
$$Z = 6*1 + 2*3 = 12\;\;\;\;P_A(\mathrm{Foregound}) = \frac{3*1 + 3}{12} = \frac{1}{2}$$

\slide{Hyper-Graphs: More General and More Concise}

A hyper-edge is a subset of nodes.

\vfill
\centerline{\includegraphics[height = 1.5in]{../images/HyperGraph}}


$$s(\hat{y}) = \sum_{i \in \mathrm{Nodes}}\; s_i[\hat{y}[i]]\; + \sum_{e \in \mathrm{Edges}}\;s_e[\hat{y}[e.i],\hat{y}[e.j]]$$

\vfill

$${\color{red} s(\hat{y}) = \sum_{e \in \mathrm{HyperEdges}}  \; s_e[\hat{y}[e]]}$$


\slide{Backpropagation}

The input is the image $x$ and the parameter package $\Phi$

\begin{eqnarray*}
 & \vdots & \\
s_e[\hat{y}] & = & \ldots \\
{\cal L} & = & - \ln\; P(y\;|\;s_{\cal E}[{\cal Y}])
\end{eqnarray*}

\vfill We abbreviate $P(\hat{y}\;|\;s_{\cal E}[{\cal Y}])$ as {\color{red} $P_s(\hat{y})$} --- the distribution on $\hat{y}$ defined by the tensor $s$.
\vfill
We need to compute {\color{red} $\nabla_s -\ln P_s(y)$}, or equivalently, {\color{red} $s_e.\grad[\tilde{y}]$}.

\ignore{
\slidetwo{Generative Fully Observed Models}
{(Language Models) are Tractable}

\centerline{\includegraphics[height=1.5in]{../images/DAG} \hspace{1in} \includegraphics[height=1.5in]{../images/SortedDAG}}

\vfill
An auto-regressive model is locally normalized.

\begin{eqnarray*}
  P_s(\hat{y}) & = & \prod_i \; P_s(\hat{y}[i]\;|\; \hat{y}[\mathrm{Parents}(i)]) \\
  \\
  P_s(\hat{y}[i]\;|\; \hat{y}[\mathrm{Parents}(i)])  & = &  \softmax_{\tilde{y}} s(\tilde{y}|\hat{y}[\mathrm{Parents}(i)])
\end{eqnarray*}

\vfill
There are exponentially many possible values for $\hat{y}$ but each softmax is over a tractable-sized set.
}

\slideplain{Back-Propagation Through An Exponential Softmax}

\bigskip
\begin{eqnarray*}
  \mathrm{loss}(s,y) & = & - \ln \left(\frac{1}{Z(s)}\;e^{s(y)}\right) \\
  \\
  & = & \ln Z(s) - s(y) \\
  \\
  \\
  s_e.\mathrm{grad}[\tilde{y}]
    & = & \left(\frac{1}{Z} \sum_{\hat{y}} e^{s(\hat{y})} \left(\partial s(\hat{y})/\partial s_e[\tilde{y}]\right)\right)
    - \left(\partial s(y) /\partial s_e[\tilde{y}]\right)
\end{eqnarray*}

\slide{Back-Propagation Through An Exponential Softmax}

\bigskip
\begin{eqnarray*}
    s_e.\mathrm{grad}[\tilde{y}]
    & = & \left(\frac{1}{Z} \sum_{\hat{y}} e^{s(\hat{y})} \left(\partial s(\hat{y})/\partial s_e[\tilde{y}]\right)\right)
    - \left(\partial s(y) /\partial s_e[\tilde{y}]\right)    \\
    \\
    & = & \left(\sum_{\hat{y}} P_s(\hat{y}) \left(\partial s(\hat{y})/\partial s_e[\tilde{y}]\right)\right)
    - \left(\partial s(y) /\partial s_e[\tilde{y}]\right)    \\
    \\
    & = & E_{\hat{y} \sim P_s}\bbone[\hat{y}[e] = \tilde{y}]
    - \mathbbm{1}[y[e] = \tilde{y}] \\
    \\
    & = & {\color{red} P_{\hat{y} \sim P_s}(\hat{y}[e] = \tilde{y})}
      - \mathbbm{1}[y[e] = \tilde{y}]
\end{eqnarray*}

\slide{Hyperedge Marginals}

\begin{eqnarray*}
    s.\mathrm{grad}[e,\tilde{y}]
    & = &  {\color{red} P_{\hat{y} \sim P_s}(\hat{y}[e] = \tilde{y})} - \bbone[y[e] = \tilde{y}] \\
\end{eqnarray*}

\vfill
We will write {\color{red} $P_e(\tilde{y})$} for {\color{red} $P_{\hat{y} \sim P_s}(\hat{y}(e) = \tilde{y})$}.

\vfill
To compute $s.\mathrm{grad}$ it suffices to compute ${\color{red} P_e(\tilde{y})}$.

\vfill
We now focus on computing the hyperedge marginals for a given hyperedge score function (MRF) $s$.

\ignore{
\slide{An Aside: Features and Weights}

The indicators $\mathbbm{1}[\hat{y}[e] = \tilde{y}]$ form a 0-1 feature vector $\Psi(\hat{y})$.

\vfill
The tensor $s[e,\;\tilde{y}]$ forms a weight vector.

\begin{eqnarray*}
s(\hat{y}) & = & \sum_e s[e,\hat{y}[e]] \\
\\
& = & \sum_{e,\tilde{y}}\;s[e,\tilde{y}] \mathbbm{1}[e, \hat{y}[e] = \tilde{y}] \\
\\
& = & f^\top \Psi(\hat{y})
\end{eqnarray*}

\slide{An Aside: Features and Weights}

The hyperedge marginals  ${\color{red} P_{\hat{y} \sim P_s}(\hat{y}[e] = \tilde{y})}$ are just the expected value of the features under the distribution
defined by the MRF.
}

\slide{Tree-Structured Models are Friendly}

\centerline{\includegraphics[height= 2in]{../images/Tree}}

\vfill
Tree structure models can always be locally renormalized to form ``autoregressive'' models that
predict one node at a time.

\vfill
Also, The hyperedge Marginals Can be Computed Exactly.
\begin{eqnarray*}
  s.\mathrm{grad}[e,\tilde{y}] & = & {\color{red} P_e(\tilde{y})} - \mathbbm{1}[y[e] = \tilde{y}]
\end{eqnarray*}


\anaslide{Defining the Messages}

\centerline{\includegraphics[height=1.5in]{../images/Tree}}

\vfill
For each edge $\{i,j\}$ and possible value $\tilde{y}$ for node $i$ we define {\color{red} $Z_{j \rightarrow i}[\tilde{y}]$}
to be  the partition function for the subtree attached to $i$ through $j$ and
with $\hat{y}[i]$ restricted to $\tilde{y}$.

\vfill
The function $Z_{j \rightarrow i}$ on the possible values of node $i$ is called the {\bf message} from $j$ to $i$.

\vfill
The reverse direction message $Z_{i \rightarrow j}$ is defined similarly.

\slide{Computing the Messages}

\centerline{\includegraphics[height=2.0in]{../images/Tree}}

\vfill
\begin{eqnarray*}
  Z_{j\rightarrow i}[\tilde{y}] & = & \sum_{\tilde{y}'}  e^{s_j[\tilde{y}'] + s_{\{j,i\}}[\{\tilde{y}',\tilde{y}\}]}
    \left(\prod_{k \in N(j),\;k \not = i}\;Z_{k\rightarrow j}[\tilde{y}']\right)
\end{eqnarray*}

\anaslide{Computing Node Marginals from Messages}

\centerline{\includegraphics[height=1.5in]{../images/Tree}}

\begin{eqnarray*}
Z_i(\tilde{y}) & \doteq & \sum_{\hat{y}:\; \hat{y}[i] = \tilde{y}} \;e^{s(\hat{y})} \\
\\
& = & e^{s_i[\tilde{y}]} \left(\prod_{j\in N(i)} Z_{j \rightarrow i}[\tilde{y}]\right) \\
\\
{\color{red} P_i(\tilde{y})} & = & Z_i(\tilde{y})/Z,\;\;\;\;\; Z = \sum_{\tilde{y}}\;Z_i(\tilde{y})
\end{eqnarray*}


\anaslide{Computing Edge Marginals from Messages}

\begin{eqnarray*}
Z_{\{i,j\}}(\tilde{y}) & \doteq & \sum_{\hat{y}:\; \hat{y}[\{i,j\}] = \tilde{y}} \;e^{s(\hat{y})} \\
\\
& = & e^{s[i,\tilde{y}[i]] + s[j,\tilde{y}[j]] +s[\{i,j\},\tilde{y}]} \\
& & \prod_{k\in N(i),\;k \not = j} Z_{k \rightarrow i}[\tilde{y}[i]] \\
& & \prod_{k\in N(j),\;k \not = i} Z_{k \rightarrow j}[\tilde{y}[j]] \\
\\
{\color{red} P_{\{i,j\}}(\tilde{y})} & = & Z_{\{i,j\}}(\tilde{y})/Z
\end{eqnarray*}

\slide{Loopy BP}

Message passing is also called belief propagation (BP).

\vfill
In a graph with cycles it is common to do {\bf Loopy BP}.

\vfill
This is done by initializing all message $Z_{i \rightarrow j}[\tilde{y}] = 1$ and then repeating (until convergence) the updates
\vfill
\begin{eqnarray*}
  P_{j \rightarrow i}[\tilde{y}] & = & \frac{1}{Z}\;Z_{j \rightarrow i}[\tilde{y}] \;\;\;\;\;Z = \sum_{\tilde{y}} Z_{j \rightarrow i}[\tilde{y}] \\
  \\
  \\
  Z_{j\rightarrow i}[\tilde{y}] & = & \sum_{\tilde{y}'}  e^{s[j,\tilde{y}'] + s[\{j,i\},\{\tilde{y}',\tilde{y}\}]}
    \left(\prod_{k \in N(j),\;k \not = i}\;P_{k\rightarrow j}[\tilde{y}']\right)
\end{eqnarray*}

\slide{Other Methods of Approximating Hyperedge Marginals}

MCMC Sampling

\vfill
Constrastive Divergence

\vfill
Pseudo-Liklihood


\slide{Sampling}
The quantities ${\color{red} P_e(\tilde{e})}$ are {\bf hyperedge marginals}.

\vfill
We can estimate the hyperedge marginals by sampling $\hat{y}$ from $P_s(\hat{y})$.

\slide{Monte Carlo Markov Chain (MCMC) Sampling}

\centerline{\bf Metropolis Algorithm}

\vfill
Pick an initial graph label $\hat{y}$ and then repeat:

\begin{enumerate}
\item Pick a ``neighbor'' $\hat{y}'$ of $\hat{y}$ uniformly at random.  The neighbor relation must be symmetric.  Perhaps Hamming distance one.

  \vfill
\item If $s(\hat{y}') > s(\hat{y})$ update $\hat{y} = \hat{y}'$

  \vfill
\item If $s(\hat{y}') \leq s(\hat{y})$ then update $\hat{y} = \hat{y}'$ with probability $e^{-(s(\hat{y}) - s(\hat{y}'))}$
  \end{enumerate}  

\slide{Markov Processes and Stationary Distributions}

A Markov process is a process defined by a fixed state transition probability $P(\hat{y}'|\hat{y}) = M_{\hat{y}',\hat{y}}$.

\vfill
Let $P^t$ the probability distribution for time $t$.

\vfill
$$P^{t+1} = MP^t$$

\vfill
If every state can be reached form every state (ergodic process) then $P^t$ converges to a unique {\bf stationary distribution} $P^\infty$

\vfill
$$P^\infty = MP^\infty$$

\slide{Metropolis Theorem}

To verify that the Metropolis process has the correct stationary distribution we simply verify that $MP = P$ where $P$
is the desired distribution.

\vfill
This can be done by checking that under the desired distribution the flow from $\hat{y}$ to $\hat{y}'$
equals the flow from $\hat{y}'$ to $\hat{y}$ ({\bf detailed balance}).

\slide{Metropolis Theorem}

For $s(\hat{y}) \geq s(\hat{y}')$

\vfill
\begin{eqnarray*}
  \mathrm{flow}(\hat{y}' \rightarrow \hat{y}) &  = & \frac{1}{Z}e^{s(\hat{y}')} \frac{1}{N} \\
  \\
\mathrm{flow}(\hat{y} \rightarrow \hat{y}') & = & \frac{1}{Z}e^{s(\hat{y})} \frac{1}{N} e^{-\Delta f} = \frac{1}{Z} e^{s(\hat{y}')} \frac{1}{N}
\end{eqnarray*}

\vfill
But detailed balance is not required in general (see Hamiltonian MCMC).

\slide{Gibbs Sampling}

The Metropolis algorithm wastes time by rejecting proposed moves.

\vfill
Gibbs sampling avoids this move rejection.

\vfill
In Gibbs sampling we select a node $i$ at random and change that node by drawing a new node value conditioned on the current values of the other nodes.

\slide{Gibbs Sampling}

\begin{eqnarray*}
  P_s(i=\tilde{y} \;|\;\hat{y}) & \doteq & P_s(\hat{y}[i] = \tilde{y}\;|\;\hat{y}[1],\ldots,\hat{y}[i-1],\hat{y}[i+1],\ldots,\hat{y}[I])
\end{eqnarray*}

\vfill
Markov Blanket Property:
$$P_s(i=\tilde{y} \;|\;\hat{y}) = P_s(i=\tilde{y} \;|\;\hat{y}[N(i)])$$
  
\vfill
Gibbs Sampling, Repeat:

\begin{itemize}
\item   Select $i$ at random

\item draw $\tilde{y}$ from $P_s(i = \tilde{y} \;|\;\hat{y})$

\item $\hat{y}[i] = \tilde{y}$
\end{itemize}

\slide{Gibbs Sampling}

Let $\hat{y}[i = \tilde{y}]$ be the assignment $\hat{y}'$ equal to $\hat{y}$ except $\hat{y}'[i] = \tilde{y}$.

\vfill
\begin{eqnarray*}
 P_s(i = \tilde{y} \;|\; \hat{y})  & = & \frac{P_s(\hat{y}[i] = \tilde{y})}{\sum_{\tilde{y}} P_s(\hat{y}[i] = \tilde{y})} \\
  \\
  \\
  & = & \frac{e^{s(\hat{y}[i = \tilde{y}])}}{\sum_{\tilde{y}} e^{s(\hat{y}[i = \tilde{y}])}}
\end{eqnarray*}

\slide{Gibbs Sampling Theorem}

$P_s(\hat{y})$ is a stationary distribution of Gibbs Sampling.

\vfill
\begin{itemize}
\item   Select $i$ at random

\item draw $\tilde{y}$ from $P_s(i = \tilde{y} \;|\;\hat{y})$

\item $\hat{y}[i] = \tilde{y}$
\end{itemize}


\vfill
The distribution before the update equals the distribution after the update.

\slide{Pseudolikelihood}

In Pseudolikelihood we replace the objective $- \log P_s(\hat{y})$ with the objective $- \log \tilde{Q}_s(\hat{y})$ where

\vfill
\begin{eqnarray*}
  \tilde{Q}_s(\hat{y}) & \doteq & \prod_i \;P_s(i = \hat{y}[i] \;|\;\hat{y}) \\
  \\
  \mathrm{loss}(f) & \doteq & - \log \tilde{Q}(y) \\
  \\
  s.\mathrm{grad}[e,\tilde{y}] & = & \sum_i - \partial \log P_s[i = \hat{y}[i] \;|\;\hat{y}] /\partial s[e,\tilde{y}]
\end{eqnarray*}


\slide{Pseudolikelihood Theorem}

$$\argmin_Q \; E_{y \sim \mathrm{Pop}} \;-\log \tilde{Q}(y) = \mathrm{Pop}$$

\vfill

\slide{Proof I}

We have

$$\min_{Q} \;E_{y \sim \mathrm{Pop}}\;-\log \tilde{Q}(y) \;\;\leq \;\; E_{y \sim \mathrm{Pop}}\;-\log \widetilde{\mathrm{Pop}}(y)$$

\vfill
If we can show

$$\min_{Q} \;E_{y \sim \mathrm{Pop}}\;-\log \tilde{Q}(y) \;\;\geq \;\; E_{y \sim \mathrm{Pop}}\;-\log \widetilde{\mathrm{Pop}}(y)$$

Then the minimizer (the argmin) is $\mathrm{Pop}$ as desired.

\slide{Proof II}

We will prove the case of two nodes.

\vfill
\begin{eqnarray*}
  & & \min_Q \;E_{y\sim \mathrm{Pop}}{-\log Q(y[1]|y[2])\;Q(y[2]|y[1])} \\
  \\
  & \geq & \min_{P_1,P_2} E_{y \sim \mathrm{Pop}}{-\log P_1(y[1]|y[2])\;P_2(y[2]|y[1])} \\
  \\
  & = & \min_{P_1} E_{y \sim \mathrm{Pop}}{-\log P_1(y[1]|y[2])} + \min_{P_2} E_{y \sim \mathrm{Pop}}{-\log P_2(y[2]|y[1])} \\
  \\
  & = & E_{y \sim \mathrm{Pop}}{-\log \mathrm{Pop}(y[1]|y[2])} + E_{y \sim \mathrm{Pop}}{-\log \mathrm{Pop}(y[2]|y[1])} \\
  \\
  & = & E_{y \sim \mathrm{Pop}}{-\log \widetilde{\mathrm{Pop}}(y|x)}
\end{eqnarray*}

  
\slideplain{Contrastive Divergence}
{\bf Algorithm (CDk)}: Run $k$ steps of MCMC for $P_s(\hat{y})$ {\bf starting from $y$} to get $\hat{y}$.

\vfill
Then set
$$s.\mathrm{grad}[e,\tilde{y}] = \mathbbm{1}[\hat{y}[e] = \tilde{y}] - \mathbbm{1}[y[e]= \tilde{y}]$$

\vfill
    {\bf CD Theorem}: If $P_s(\hat{y}) = \mathrm{Pop}$ then
    
    $$E_{y \sim \mathrm{Pop}}\; \mathbbm{1}[\hat{y}[e] = \tilde{y}] - \mathbbm{1}[y[e]= \tilde{y}] = 0$$

\vfill
{\bf Here we can take $k=1$ --- \bf no mixing time required}.


\slide{END}

}
\end{document}
 


\slide{Colorization --- a Self-Supervised Problem}

\centerline{\includegraphics[height = 2in]{../images/colorization}}

$x$ is a black and white image.

\vfill
$y$ is a ``color'' image drawn from $\pop(y|x)$ where the color has been rounded to one of, say, 100 color words.

\vfill
$\hat{y}$ is an arbitrary color image.

\vfill
$P_\Phi(\hat{y}|x)$ is the probability that model $\Phi$ assigns to the color image $\hat{y}$ given black and white image $x$.

\slide{Colorizing Superpixels}
\centerline{\includegraphics[height = 3in]{../images/SLIC} \hspace{.5in} \includegraphics[height = 3in]{../images/SLICcolor}}
\centerline{\huge SLIC superpixels, Achanta et al.}

\vfill
$\hat{y}[i]$ is the color value of superpixel $i$ in color image $\hat{y}$.

\vfill
$\hat{y}[(i,j)]$ is the pair $(\hat{y}[i],\hat{y}[j])$ for neighbors $i$ and $j$.

\slide{Exponential Softmax}

\begin{eqnarray*}
P_\Phi(\hat{y}|x) & = & \softmax_{\hat{y}}\;s_\Phi(\hat{y}|x)
\end{eqnarray*}

\vfill
Let ${\cal C}$ be colors, ${\cal I}$ be superpixels, and ${\cal E}$ be edges.

\vfill
We will compute

\vfill {\bf a unary potential tensor} $s_i[c] = s_\Phi(c|x,i)$

\vfill
{\bf a binary potential tensor} $s_e[c,c'] = s_\Phi(c,c'|x,e)$

\vfill
$$s_\Phi(\hat{y}|x) = \sum_{i \in {\cal I}} s_i[\hat{y}[i]] + \sum_{e \in {\cal E}}\;s_e[\hat{y}[e.i],\hat{y}[e.j]]$$
