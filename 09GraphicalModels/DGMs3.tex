\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2019}
  \vfill
  \vfill
  \centerline{\bf Connectionist Temporal Classification (CTC)}
  \vfill
  \centerline{\bf A Latent Variable Deep Graphical Model}
  \vfill
  \centerline{\bf The General Expectation Gradient (EG) Algorithm}
\vfill
\slide{The Big Picture I}

Conditional vs. Unconditional

\vfill
$$\Phi^* = \argmin_\Phi E_{(x,y) \sim \mathrm{Pop}}\;-\ln \;P(y|x)$$

\vfill
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Pop}}\;-\ln \;P(y)$$

\vfill
This is a non-distinction: the issues in the to the conditional case
are exactly the same as in the unconditional case.

\slide{The Big Picture II}

The binary case: $y \in \{-1,1\}$ (cancer screening).

\vfill
The multiclass case: $y \in {\cal Y}$ where iteration over $\hat{y} \in {\cal Y}$ is feasible (MNIST, CFAR, ImageNet).

\vfill
{\color{red} The structured case: $y \in {\cal Y}$ where ${\cal Y}$ is discrete but iteration over $\hat{y} \in {\cal Y}$ is infeasible} (language modeling, speech recognition).

\slide{Big Picuture III}

Graphical models (such as CTC) rely on assumptions about the structure of $P_\Phi(y)$.

\vfill
The {\color{red} expectation gradient (EG)} algorithm applies when $P_\Phi(y)$ can be computed exactly using dynamic programming.

\vfill
{\color{red} Requiring that $P_\Phi(y)$ is computable restricts the model but is justified in some cases (such as CTC).}

\slidetwo{Connectionist Temporal Classification (CTC)}
{Phonetic Transcription}

A speech signal
$$x = x_1,\; \ldots,\; x_T$$
is labeled with a phone sequence
$$y = y_1, \ldots, y_N$$
with $N << T$ and with $y_n \in {\cal Y}$ for a set of phonemes ${\cal Y}$.

\vfill
The length $N$ of $y$ is not determined by $x$ and the alignment between $x$ and $y$ is not given.

\slide{CTC}
The model defines $P_\Phi(\hat{z}|x)$ where $\hat{z}$ is latent and where $y$ is determined by $\hat{z}$.

\vfill
$$\hat{z} = \hat{z}_1,\;\ldots,\;\hat{z}_T,\;\;\;\hat{z}_t \in {\cal Y} \cup \{\bot\}$$

\vfill
The sequence

\vfill
$$y(\hat{z}) = y_1,\;\ldots,\;y_N$$

\vfill
is the result of removing all the occurrences of $\bot$ from $\hat{z}$.

$$\bot,a_1,\bot,\bot,\bot,a_2,\bot,\bot,a_3,\bot \Rightarrow a_1,a_2,a_3$$

\slide{The CTC Model}

\begin{eqnarray*}
  h_1,\;\ldots,\;h_T & = & \mathrm{RNN}_\Phi(x_1,\;\ldots,\;x_T) \\
  \\
  \\
  P_\Phi(\hat{z}_t|x_1,\ldots,x_T) & = & \softmax_{\hat{z}_t} \;e(\hat{z}_t)^\top h_t
\end{eqnarray*}

\vfill
Where $e(\hat{z}_t)$ is a vector embedding of the phoneme $\hat{z}_t$.  The embedding is a parameter of the model.

\vfill
Note that $\hat{z}_1$, $\ldots$ $\hat{z}_T$ are all independent given $x$.

\slide{The Expectation Gradient (EG) Algorithm}

CTC is a special case of a latent variable graphical model.

\vfill
In cases where $P_\Phi(y|x)$ can be computed from dynamic programming
we can backpropagate through the dynamic programming algorithm to get the gradient of cross-entropy loss.

\vfill
This general technique is the {\bf expectation gradient} (EG) algorithm.

\vfill
We will first show that for the CTC model we can compute $P_\Phi(y|x)$ by
dynamic programming.

\vfill
We will then note that it is not necessary to backpropagate through the dynamic programming algorithm.

\slideplain{Dynamic Programming (Forward-Backward)}
\begin{eqnarray*}
  x & = & x_1,\;\ldots,\; x_T \\
  \hat{z} & = & \hat{z}_1, \;\ldots,\;\hat{z}_T,\;\;\;\hat{z}_t \in {\cal Y}\cup \{\bot\} \\
  y & = & y_1,\;\ldots,\;y_N,\;\;\;\;y_n \in {\cal Y},\;\;\;N << T \\
  y(\hat{z}) & = & (\hat{z}_1,\;\ldots,\;\hat{z}_T) - \bot
\end{eqnarray*}

\vfill

Forward-Backward
\begin{eqnarray*}
  \vec{y}_t & = & (\hat{z}_1,\;\ldots,\;\hat{z}_t)-\bot \\
  F[n,t] & = & P(\vec{y}_t = y_1,\;\ldots,\;y_n) \\
  B[n,t] & = & P(y_{n+1},\;\ldots,y_N | \vec{y}_t = y_1,\;\ldots,\;y_n) \\
  P(y) & = & F[N,T] = B[0,0]
\end{eqnarray*}



\slide{Dynamic Programming (Forward-Backward)}

\begin{eqnarray*}
  \vec{y}_t & = & (\hat{z}_1,\;\ldots,\;\hat{z}_t)-\bot \\
  F[n,t] & = & P(\vec{y}_t = y_1,\;\ldots,\;y_n) \\
  B[n,t] & = & P(y_{n+1},\;\ldots,y_N | \vec{y}_t = y_1,\;\ldots,\;y_n)
\end{eqnarray*}

\begin{eqnarray*}
  F[0,0] & = & 1 \\
  F[n,0] & = & 0 \;\;\;\mbox{for $n > 0$} \\
  F[n+1,t+1] & = & P(\hat{z}_{t+1} = \bot) F[n+1,t] + P(\hat{z}_{t+1} = y_{n+1})F[n,t] \\
  \\
  B[N,T] & = & 1 \\
  B[n,T] & = & 0,\;\;\;\mbox{for $n < N$} \\
  B[n-1,t-1] & = & P(\hat{z}_t = \bot)B[n-1,t] +  P(\hat{z}_t = y_n)B[n,t]
\end{eqnarray*}

\slideplain{The Big Picture I}

Conditional vs. Unconditional

\vfill
$$\Phi^* = \argmin_\Phi E_{(x,y) \sim \mathrm{Pop}}\;-\ln \;P(y|x)$$

\vfill
$$\Phi^* = \argmin_\Phi E_{y \sim \mathrm{Pop}}\;-\ln \;P(y)$$

\vfill
This is a non-distinction: the issues in the to the conditional case
are exactly the same as in the unconditional case.

\slide{The Big Picture II}

The binary case: $y \in \{-1,1\}$ (cancer screening).

\vfill
The multiclass case: $y \in {\cal Y}$ where iteration over $\hat{y} \in {\cal Y}$ is feasible (MNIST, CFAR, ImageNet).

\vfill
{\color{red} The structured case: $y \in {\cal Y}$ where ${\cal Y}$ is discrete but iteration over $\hat{y} \in {\cal Y}$ is infeasible} (language modeling, speech recognition).

\vfill
{\color{red} Graphical models (such as CTC) rely on assumptions about the structure of $P_\Phi(y)$.}

\slide{END}

}
\end{document}

