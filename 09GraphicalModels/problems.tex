\documentclass{article}
\input ../preamble
\parindent = 0em

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}

\bigskip

\centerline{\bf Problems For Fundamental Equations.}

\bigskip
\bigskip
{\bf Problem 4.}
Consider a graphical model with $N$ nodes numbered 1 through $N$ and where each node can take on one of the values $0$ or $1$.
We let $\hat{x}$ be an assignment of a value to every node. We define the score of $\hat{x}$ by
$$f(\hat{x}) = \sum_{i = 1}^{N-1} \mathbbm{1}[\hat{x}[i] = \hat{x}[i+1]]$$
The probability distribution over assignments is defined by a softmax.
$$Q_f(\hat{x}) = \softmax_{\hat{x}} f(\hat{x})$$
What is the {\bf Pseudoliklihood} of the all ones assignment?

\solution{
$$\tilde{P}_f(\hat{x}) = \Pi_i P_f(\hat{x}[i] \;|\; \hat{x}/i)$$

where $\hat{x}/i$ consists of all components of $\hat{x}$ other than $i$.  In a graphical model $P_f(\hat{x}[i] \;|\; \hat{x}/i)$ is determined by the neighbors of $i$ and we can consider only how a value is scored against it neighbors.  For $\hat{x}$ equal to all ones we have

\begin{eqnarray*}
f(\hat{x}) & = & N-1 \\
\\
f(\hat{x}[i = 0]) & = & \left\{\begin{array}{ll} N-3 & \mbox{for $1 < i < N$} \\ N-2 & \mbox{for $i=1$ or $i=N$} \end{array}\right.
\end{eqnarray*}
For $1 < i < N$ we get
\begin{eqnarray*}
Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) & = & \frac{e^{N-1}}{e^{N-1} + e^{N-3}} \\
\\
& = & \frac{1}{1 + e^{-2}}
\end{eqnarray*}
and for $i = 1$ or $i = N$ we get
$$Q_f(\hat{x}[i=1]\; |\;\hat{x}/i) = \frac{1}{1 + e^{-1}}$$

This gives

$$\tilde{Q}(\hat{x}) = (1 + e^{-1})^{-2}(1 + e^{-2})^{-(N-2)}$$
}

  
\end{document}
