 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Winter 2019</p>

<font color = "red">
  <p>Quiz Solutions: <a href = quiz1/quiz1.pdf>quiz 1</a>  <a href = quiz2/quiz2.pdf>quiz 2</a> <a href = quiz3/quiz3.pdf>quiz 3</a> <a href = quiz4/quiz4.pdf>quiz 4</a></p>
</font>

<!--
<p><font color = "red">I have reworked the slides on graphical models
and believe that they are now much more intelligable.</font></p>
-->

<p> This year the course will not involve programming assignments or class projects.  There will be problem sets but the grade will be based entirely on exams including a final. Exams will include problems sampled from the problem sets plus new problems. I will generally give permission to take the class but prospective students might want to look at the first lecture slides and the associated problems to get a sense of the level of mathematical maturity assumed. </p>

<p> The course will involve reading and writing
pseudo-code corresponding to code in frameworks such as PyTorch. This is analogous to the use of
pseudo-code in an algorithms class as distinct from actual programming in a programming class.</p>

<p>This course covers the topics listed below.  Most topics are relevant to most applications ---
applications to natural language processing, computer vision, speech recognition, computational
biology, and computational chemistry will be integrated into the presentations of the general
methods.</p>

<ol>
<li>Information theory: entropy, cross-entropy, KL-divergence, mutual information.</li>

<li>Deep learning frameworks: computation graphs, back-propagation, minibatching.</li>

<li>Basic Architectures: multi-layer perceptrons, convolutional neural
networks, Einstein notation.</li>

<li>More advanced architectures: gated RNNs (LSTMs), ResNet, attention.</li>

<li>Stochastic gradient descent (SGD): standard variations (Vanilla, Adam, RMSProp), minibatch
scaling laws, second order methods, Hessian-vector products, SGD-friendly initialization.</li>

<li>Generalization and Regularization: PAC-Bayesian generalization bounds, L2 regularization
(shrinkage), dropout.</li>

<li>Autoencoders: rate-distortion autoencoding, variational autoencoding (VAEs) and the evidence
lower bound (the ELBO), vector quantized VAEs (VQ-VAE).</li>

<li>Deep graphical models: expectation maximization (EM), expectation gradient (EG), connectionsist
temporal classification (CTC), various EG approximations.</li>

<li>Generative Adversarial Networks (GANs): Adversarial optimization, Jensen-Shannon divergence,
mode collapse, Wasserstein GANs, progressive GANs.</li>

<li>Deep Reinforcement Learning: The REINFORCE algorithm, policy-gradient theorems, DQN, A3C,
AlphaZero.</li>

</ol>


Exam Schedule:
								  
<ol>

  <li> Tuesday, January 15, 10% of grade, class 3</li>
  <li> Tuesday, January 29, 20% of grade, class 7</li>
  <li> Tuesday, February 12, 20% of gradem=, class 11</li>
  <li> Tuesday, February 26, 20% of grade, class 15</li>
  <li> Final, Tuesday, March 19, 1:30-3:30, TTI 526B, 30% of grade</li>

</ol>

Office Hours: Mondays 9:00-11:00, TTIC 530 and 1:00-3:00, TTIC 435

<p>Lectures Slides and Course Material (under development --- please refresh for latest version):</p>

<ol>

  <li><a href = 01intro/intro.html> Information Theory: The Fundamental Equations of Deep Learning</a></li>

  <li><a href = 02MLP/Backprop.html> Back-Propagation and Frameworks</li>

  <li><a href = 03CNNs/CNNs.html> Convolutional Neural Networks (CNNs)</li>

  <li><a href = 04Highway/highway.html> Trainability: Initialization, Batch Normalization, ResNet and Gated RNNs </li>

  <li><a href = 05RNNs/LangModel.html> Language Modeling, Machine Translation and Attention</li>

  <li><a href = 06SGD/SGD.html> First Order Stochastic Gradient Descent (SGD)</li>

  <li><a href = 07regularization/regularization.html> Generalization and Regularization</li>

  <li><a href = 09GraphicalModels/CTC.html> Connectionist Temporal Classification (CTC)</li>

  <li><a href = 09GraphicalModels/DGMs.html> Deep Graphical Models</li>

  <li><a href = 11AutoEncoders/info.html> More Information Theory: Compression as a Cross-Entropy Guarantee and Avoiding Differential Entropy </li>
  
  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders (RDAs)</li>

  <li><a href = 11AutoEncoders/Variational.html> Expectation Maximiztion (EM), The Evidence Lower Bound (The ELBO) and Variational Autoencoders (VAEs)</li>

  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</a></li>

  <li><a href = pretraining/pretraining.html> Pretraining</a></li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</a></li>

  <li><a href = 16alpha/alpha.html> AlphaZero</a></li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </a></li>

  <li><a href = 17Interpretation/interp.html> The Black Box Problem</a></li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</li>
  
</ol>
