 <title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Winter 2019</p>

<p> This year the course will not involve programming assignments or class projects.  There will be problem sets but the grade will be based entirely on exams including a final. Exams will include problems sampled from the problem sets plus new problems. I will generally give permission to take the class but prospective students might want to look at the first lecture slides and the associated problems to get a sense of the level of mathematical maturity assumed. </p>

<p> The course will involve reading and writing
pseudo-code corresponding to code in frameworks such as PyTorch. This is analogous to the use of
pseudo-code in an algorithms class as distinct from actual programming in a programming class.</p>

<p>This course covers the topics listed below.  Most topics are relevant to most applications ---
applications to natural language processing, computer vision, speech recognition, computational
biology, and computational chemistry will be integrated into the presentations of the general
methods.</p>

<ol>
<li>Information theory: entropy, cross-entropy, KL-divergence, mutual information.</li>

<li>Deep learning frameworks: computation graphs, back-propagation, minibatching.</li>

<li>Basic Architectures and Einstein Notation: multi-layer perceptrons, Convolutional Neural
Networks (Alexnet).  Einstein notation as a framework-independent representation.</li>

<li>More advanced architectures: gated RNNs (LSTMs), ResNet, attention.</li>

<li>Stochastic gradient descent (SGD): standard variations (Vanilla, Adam, RMSProp), minibatch
scaling laws, second order methods, Hessian-vector products, SGD-friendly initializat on.</li>

<li>Generalization and Regularization: PAC-Bayesian generalization bounds, L2 regularization
(shrinkage), dropout.</li>

<li>Autoencoders: rate-distortion autoencoding, variational autoencoding (VAEs) and the evidence
lower bound (the ELBO), vector quantized VAEs (VQ-VAE).</li>

<li>Deep graphical models: expectation maximization (EM), expectation gradient (EG), connectionsist
temporal classification (CTC), various EG approximations.</li>

<li>Generative Adversarial Networks (GANs): Adversarial optimization, Jensen-Shannon divergence,
mode collapse, Wasserstein GANs, progressive GANs.</li>

<li>Deep Reinforcement Learning: The REINFORCE algorithm, policy-gradient theorems, DQN, A3C,
AlphaZero.</li>

</ol>


Exam Schedule:
								  
<ol>

  <li> Tuesday, January 15, 10% of grade</li>
  <li> Tuesday, January 29, 20% of grade</li>
  <li> Tuesday, February 12, 20% of grade</li>
  <li> Tuesday, February 26, 20% of grade</li>
  <li> Final, to be determined, 30% of grade</li>

</ol>

<p>Lectures Slides and Course Material (incomplete and under development):</p>

<ol>

  <li><a href = 01intro/intro.html> The Fundamental Equations of Deep Learning</a></li>

  <li><a href = 02MLP/Backprop.html> Back-Propagation and Frameworks</li>  

  <li><a href = 03CNNs/CNNs.html> Convolutional Neural Networks (CNNs)</li>

  <li><a href = 04Highway/highway.html> Controling Gradients: Initialization, Batch Normalization, ResNet and Gated RNNs </li>

  <li><a href = 05RNNs/LangModel.html> Language Modeling and Machine Translation</li>

<!--  <li><a href = 06SGD/SGD.html> First Order Stochastic Gradient Descent (SGD)</li>

  <li><a href = 07regularization/regularization.html> Regularization</li>

  <li><a href = 17Interpretation/interp.html> Interpretation</li>

  <li><a href = 09GraphicalModels/DGMs.html> Fully Observed Graphical Models I: Exponential Softmax, Sufficient Statistics, and Belief Propagation </li>

  <li><a href = 09GraphicalModels/DGMs2.html> Fully Observed Graphical Models II: Approximate SGD Algorithms</li>

  <li><a href = 09GraphicalModels/DGMs3.html> Partially Observed Graphical Models: Expectation Maximization (EM), Expected Gradient (EG), and CTC</li>

  <li><a href = 11AutoEncoders/Variational.html> Variational Autoencoders (VAEs)</li>

  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders</li>

  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</li>

  <li><a href = 16alpha/alpha.html> AlphaZero</li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</li>
  -->
    
</ol>
