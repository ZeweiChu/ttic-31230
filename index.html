<title>TTIC 31230: Fundamentals of Deep Learning</title>

<header>TTIC 31230: Fundamentals of Deep Learning</header>

<p> David McAllester</p>

<p> Winter 2019</p>

<b>Warning:</b> This year the course is going to be more theoretical than in previous years.  There will be problem sets but the grade will be based entirely on exams including a final. There will be no programming project. There will be extensive use of information theory and tensor calculus.  I will generally give permission to take the class but will give an exam after two lectures to help people determine if they have an adequate background (no one will be ejected from the class).  The following lecture slides will be revised as the course proceeds.  The course material will not include all the material in these slides.

<ol>

  <li><a href = 01intro/intro.pdf> Introduction and Historical Notes</a></li>

  <li><a href = 08InfoTheory/information.html> Information Theory</li>

  <li><a href = 02MLP/MLPa.pdf> Multi-Layer Perceptrons (MLPs) and Stochastic Gradient Descent (SGD)</a></li>
  
  <li><a href = 02MLP/Backprop.html> Feed-Forward Computation Graphs, Backpropagation, and the Educational Framework (EDF) </li>

  <li><a href = 03CNNs/CNNs.html> Convolutional Neural Networks (CNNs) </li>

  <li><a href = 03CNNs/CNNb.html> Invariant Theory </li>

  <li><a href = 04Highway/highway.html> Controling Gradients: Initialization, Batch Normalization, Resnets and Gated RNNs </li>

  <li><a href = 05RNNs/LangModel.html> Language Modeling and Machine Translation</li>

  <li><a href = 06SGD/SGD.html> First Order Stochastic Gradient Descent (SGD)</li>

  <li><a href = 13SGD2/SGD2.html> Gradients as Dual Vectors, Hessian-Vector Products, and Information Geometry </li>

  <li><a href = 07regularization/regularization.html> Regularization</li>

  <li><a href = 17Interpretation/interp.html> Interpretation</li>

  <li><a href = 09GraphicalModels/DGMs.html> Fully Observed Graphical Models I: Exponential Softmax, Sufficient Statistics, and Belief Propagation </li>

  <li><a href = 09GraphicalModels/DGMs2.html> Fully Observed Graphical Models II: Approximate SGD Algorithms</li>

  <li><a href = 09GraphicalModels/DGMs3.html> Partially Observed Graphical Models: Expectation Maximization (EM), Expected Gradient (EG), and CTC</li>

  <li><a href = 11AutoEncoders/Variational.html> Variational Autoencoders (VAEs)</li>

  <li><a href = 11AutoEncoders/Rate.html> Rate-Distortion Autoencoders</li>

  <li><a href = 14GANs/GANs.html> Generative Adversarial Networks (GANs)</li>

  <li><a href = 15RL/RL.html> Reinforcement Learning (RL)</li>

  <li><a href = 16alpha/alpha.html> AlphaZero</li>

  <li><a href = 18AGI/AGI.html> The Quest for Artificial General Intelligence (AGI)</li>



  

  

</ol>
								  
