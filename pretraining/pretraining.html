<header>TTIC 31230: Fundamentals of Deep Learning, Winter 2019</header>

<p>Pretraining</p>

<p>Lecture Slides in Preparation</p>

<strong> Supervised Pretraining for Vision:</strong>

<p><a href = https://arxiv.org/abs/1311.2524>Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</a>
  , Girshick at al., 2013</p>

<p><a href = https://arxiv.org/abs/1805.00932/>Exploring the Limits of Weakly Supervised Pretraining</a>, Mahajan et al., 2018</p>

<p><a href = https://arxiv.org/abs/1811.08883/>Rethinking ImageNet Pretraining</a>, He et al., 2018 </p>

<strong> Unsupervised Pretraining for Vision: </strong>

<p><a href = https://arxiv.org/abs/1604.07379/>Context Encoders: Feature Learning by Inpainting</a>, Pathak et al., 2016</p>

<p><a href = https://arxiv.org/abs/1603.06668/>Learning Representations for Automatic Colorization</a>, Larsson et al., 2016</p>

<p><a href = https://arxiv.org/abs/1807.03748/>Representation Learning with Contrastive Predictive Coding</a> (CPC), van den Oord et al, 2018</p>

<p><a href = https://arxiv.org/abs/1808.06670/>Learning Deep Representations by Mutual Information Estimation and Maximization</a>, Hjelm et al., 2018</p>

<strong> Unsupervised Pretraining for Language: </strong>

<p><a href = https://arxiv.org/abs/1301.3781> Efficient Estimation of Word Representations in Vector Space</a> (Word2Vec), Mikolov et al., 2013</p>

<p><a href = https://arxiv.org/abs/1712.09405/>Advances in Pre-Training Distributed Word Representations</a> (FastText, BPE), Mikolov et al., 2017</p>

<p><a href = https://arxiv.org/abs/1706.03762>Attention is All You Need</a> (The Transformer), Vaswani et al., 2017</p>

<p><a href = https://arxiv.org/abs/1802.05365/>Deep contextualized word representations</a> (ELMO), Peters et al., 2018</a></p>

<p><a href = https://arxiv.org/abs/1810.04805>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>, (BERT) Devlin et al., 2018</p>

<p><a href = https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>
    Language Models are Unsupervised Multitask Learners</a>, (GPT-2) Radford et al., 2019</p>

<strong> Unsupervised Machine Translation (UMT): </strong>

<p><a href = https://openreview.net/pdf?id=Sy2ogebAW> Unsupervised Nerual Machine Translation</a>, Artexte et al., 2018</p>

<p><a href = https://openreview.net/pdf?id=rkYTTf-AZ> Unsupervised Machine Translation Using Monolingual Corpora Only</a>, Lample et al., 2018</p>

<p><a href = https://arxiv.org/abs/1902.01313>An Effective Approach to Unsupervised Machine Translation</a>, Artexte et. al., 2019</p>

  
<strong>Mutual Information Pretraining:</strong>

<p><a href = https://arxiv.org/abs/1802.07572/>Information Theoretic Co-Training</a>, McAllester, 2018</p>

<p><a href = https://arxiv.org/abs/1811.04251/>Formal Limitations on the Measurement of Mutual Information</a>, McAllester and Stratos, 2018</p>
