\input ../SlidePreamble
\input ../preamble


\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2019}
  \vfill
  \centerline{Information Theory Revisited}
  \vfill
  \centerline{Shannon's Source Coding Theorem}
  \vfill
  \centerline{Avoiding Differential Entropy}

\slide{Measuring Cross Entropy of an Exponential Softmax}

We typically cannot measure cross-entropy for a graphical model.

\vfill
Although we can train using pseduo-likelihood, it remains unclear how to measure the resulting cross-entropy loss.

\vfill
One approach is to construct a compression algorithm.

\vfill
Let $|z_\Phi(y)|$ be the bit length of the compression of $y$.
{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop} \;-\ln P_\Phi(y) \\
\\
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop} \;|z_\Phi(y)|\;\;\;\mbox{such that $\forall y\;\;\;y = y_\Phi(z_\Phi(y))$}
\end{eqnarray*}
}

\slide{Sparse Labeling Compression (TZ)}

After training a graphical model $\Phi$ on semantic segmentations we can code a segmentation $y$
by a sparse segmentation $z_\Phi(y)$ assigning a label to only a small fraction of the pixels.

\vfill
We then define the decoding $y_\Phi(z_\Phi(y))$ to be the result of running deterministic local search over
the labels of the unspecified pixels to find a locally best-scoring full semantic segmentation.

\vfill
We can define $z_\Phi(y)$ by some heuristic approximation to
$$z_\Phi(y) = \argmin_{z:\; y_\Phi(z) = y}\;|z|$$
where $|z|$ is the number of pixels assigned by $z$.

\slide{Entropy and Compressibility}

Let $S$ be a finite set.

\vfill
Let $z$ be a compression (or coding) function  assigning a bit string $z(y)$ to each $y \in S$.

\vfill
The compression function $z$ is called {\em prefix-free} if for $y' \not = y$ we have that $z(y')$ is not a prefix of $z(y)$.

\vfill
Null-terminated byte strings are prefix-free bit strings.

\slide{Prefix-Free Codes as Probabilities}

A prefix-free code defines a binary branching tree --- branch on the first code bit, then the second, and so on.

\vfill
For a prefix-free code, only the leaves of this tree can be labeled with the elements of $S$.

\vfill
The code defines a probability distribution on $S$ by randomly selecting branches.

\vfill
We have $P_z(y) = 2^{-|z(y)|}$.

\slide{The Source Coding (compression) Theorem}

(1) There exists a prefix-free code $z$ such that
$$|z(y)| <= (- \log_2 \mathrm{Pop}(y)) + 1$$
and hence
$$E_{y\sim \mathrm{Pop}} |z(y)| \leq H_2(\mathrm{Pop}) +1$$

\vfill
(2) For any prefix-free code $z$

$$E_{y \sim \mathrm{Pop}}\;|z(y)| \geq H_2(\mathrm{Pop})$$

\slide{Code Construction}

\vfill
We construct a code by iterating over $y \in S$ in order of decreasing probability (most likely first).

\vfill
For each $y$ select a code word $z(y)$ (a tree leaf) with length (depth)

\vfill
$$|z(y)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$$

\vfill
and where $z(y)$ is not an extension of (under) any previously selected code word.

\slide{Code Existence Proof}

At any point before coding all elements of $S$ we have
$$\sum_{y \in \mathrm{Defined}} 2^{-|z(y)|} \leq \sum_{y \in \mathrm{Defined}} \mathrm{Pop}(y) < 1$$

\vfill
Therefore there exists an infinite descent into the tree that misses all previous code words.

\vfill
Hence there exists a code word $z(x)$ not under any previous code word with
$|z(x)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$.

\vfill
Furthermore $z(x)$ is at least as long as all previous code words and hence $z(x)$ is not a prefix of any previously selected code word.

\slide{No Better Code Exists}

Let $z$ be an arbirtary coding.

\begin{eqnarray*}
E_y\;|z(y)| & = & E_y\; -\log_2 P_z(y) \\
\\
 & = & H_2(\mathrm{Pop},P_z) \\
 \\
 &= & H_2(\mathrm{Pop}) + KL_2(\mathrm{Pop},P_z) \\
 \\
 &\geq & H_2(\mathrm{Pop})
\end{eqnarray*}
 
\slide{Huffman Coding}

Maintain a list of trees $T_1,\;\dots,\;T_N$.

\vfill
Inititally each tree is just one root node labeled with an element of $S$.

\vfill
Each tree $T_i$ has a weight equal to the sum of the probabilities of the nodes on the leaves of that tree.

\vfill
Repeatedly merge the two trees of lowest weight into a single tree until all trees are merged.

\slide{Optimality of Huffman Coding}

{\bf Theorem}: The Huffman code $T$ for $\mathrm{Pop}$ is optimal --- for any other tree $T'$ we have $H(\mathrm{Pop},T) \leq H(\mathrm{Pop},T')$.

\vfill
{\bf Proof}: The algorithm maintains the invariant that there exists an optimal tree including
all the subtrees on the list.

\vfill
To prove that a merge operation maintains this invariant we consider any tree containing the given subtrees.

\vfill
Consider the two subtrees $T_i$ and $T_j$ of minimal weight.  Without loss of generality we can assume that $T_i$ is at least as deep as $T_j$.

\vfill
Lowering $T_j$ to be the sibling of $T_i$ while raising the old sibling of $T_i$ to $T_j$'s original position
brings $T_i$ and $T_j$ together and can only improve the average depth.

\slide{Avoiding Differential Entropy}

Consider a continuous density $p(x)$.  For example

\vfill
$$p(x) = \frac{1}{\sqrt{2\pi}\; \sigma}\; e^{\frac{-x^2}{2\sigma^2}}$$

\vfill
Differential entropy is often defined as

\vfill
$$H(p) \doteq \int \left(\ln \frac{1}{p(x)}\right) p(x) dx$$


\slide{Differential Entropy Depends on the Choice of Units}

\begin{eqnarray*}
  H({\cal N}(0,\sigma)) & = &  + \int \left( \ln(\sqrt{2\pi} \sigma) + \frac{x^2}{2\sigma^2}\right) p(x) dx \\
  \\
  & = & \ln \sigma + \ln \sqrt{2\pi} + \frac{1}{2}
\end{eqnarray*}

But the numerical value of $\sigma$ depends on the choice of units.

\vfill A distributions on lengths will have a different entropy
when measuring in inches than when measuring in feet.

\vfill
Also, for $\sigma$ small we get $H({\cal N}(0,\sigma)) < 0$

\vfill

\slide{More Problems with Differential Entropy}

There are also other problems with continuous entropy and cross-entropy.

\vfill
\begin{itemize}
\item Differential entropy violates the source coding theorem --- it takes an infinite number of bits to code a real number.

\vfill
\item Differential entropy violates the data processing inequality that $H(f(x)) \leq H(x)$.  For a continuous random variable $x$ under finite continuous entropy we can have $H(f(x)) > H(x)$.
\end{itemize}

\vfill
For these reasons it seems advisable to avoid differential entropy and differential cross entropy.

\slide{Differential KL-divergence is Independent of Units}

$$KL(p,q) = \int \left( \ln \frac{p(x)}{q(x)}\right) p(x) dx$$

\vfill
If $x$ has units of length then $p(x)$ has units of probability mass per length.

\vfill
In this case $p(x)/q(x)$ is dimensionless.


\slide{Avoiding Differential Entropy}

To avoid differential entropy we can use a rate-distortion objective.

\vfill
{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop} \;-\ln P_\Phi(y)\;\;\;\mbox{$y$ discrete} \\
\\
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop}\left(\begin{array}{l}\;\;\;\;-\ln P_\Phi(z_\Phi(y)) \\ + \lambda \mathrm{Dist}(y,y_\Phi(z_\Phi(y)))\end{array}\right)
\left\{\begin{array}{l}\mbox{$y$ continuous} \\ \mbox{$z$ discrete}\end{array}\right.
\end{eqnarray*}
}

\slide{Lossy Compression}

Lossy compression combines compression for measuring cross-entropy with distortion for avoiding differential entropy.

\vfill
{\color{red}
\begin{eqnarray*}
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop} \;-\ln P_\Phi(y)\;\;\mbox{$y$ discrete} \\
\\
\Phi^* & = & \argmin_\Phi \;E_{y \sim \pop}\left(\begin{array}{l} \;\;\;\;|\tilde{z}_\Phi(y)| \\ + \lambda \mathrm{Dist}(y,y_\Phi(\tilde{z}_\Phi(y))) \end{array}\right)
\left\{\begin{array}{l}\mbox{$y$ continuous} \\ \mbox{$\tilde{z}$ discrete}\end{array}\right.
\end{eqnarray*}
}

\slide{END}

}
\end{document}

